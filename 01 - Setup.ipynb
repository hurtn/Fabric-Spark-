{"cells":[{"cell_type":"markdown","source":["# Batch or continuous ingestion of incremental data using Spark Structure Streaming\n","\n","Scenario: You wish to maintain a \"copy\" of source system tables in your lakehouse. An incremental feed of changes is published as files (parquet) to OneLake. You need to merge these changes (inserts, updates & deletes) to target tables as an incremental operation i.e. only process new changes/files which have arrived. The process needs to keep the target tables in sync either in near real-time or frequently (hourly, daily) to support downstream ETL pipelines.  \n","\n","Session outcomes: Demonstrate how to merge incremental data feeds from OneLake into target Lakehouse tables using Spark Structured Streaming.\n","\n","![Overview](https://github.com/hurtn/Fabric-Spark-/blob/main/Overview.png?raw=true)\n","\n","What is Spark Structured Streaming: Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming/incremental data continues to arrive. For more information on strucuted streaming please refer to the [Apache Spark documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).\n","The engine uses a mechanism called checkpointing to keep track of which files have already been processed. This means that even in the event of failure, Structured Streaming can unsure end-to-end exactly-once semantics whereby each file will be processed only once. The main notebook can also be run in both batch (scheduled) or continuous (micro-batch) mode based on the [streaming trigger](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers) configuration and will utilise checkpoints to only process new data.\n","\n","To get started: Ensure you have imported the following three notebooks (including this one) into your workspace. A summary of the notebooks is as follows:\n","\n","01 - Setup: This notebook will guide you through the setup by creating Lakehouse tables based on WWI sample data as part of the [lakehouse tutorial](https://learn.microsoft.com/en-us/fabric/data-engineering/tutorial-lakehouse-data-ingestion). It will also allow you to simulate the arrival of new incremental data and verify that data has been merged into the target tables.\n","\n","02 - Orchestrator: The notebook orchestrates the ingestion process. A definition of tables and related metadata is dynamically generated (DAG) which is submitted as a set of instructions to the [runMulitple Spark utility](https://learn.microsoft.com/en-us/fabric/data-engineering/microsoft-spark-utilities#reference-run-multiple-notebooks-in-parallel). The DAG runs the referenced notebook (03 - TableLoader) for each table, detecting new data to merge into the Lakehouse table. \n","\n","03 - TableLoader: An instance of this notebook runs for each table (concurrently) merging new incremental data/files with the associated target Lakehouse table. Depending on the trigger configuration, it will run either once for all tables per schedule before handing back control to the Orchestrator notebook, or it will run continuously until terminated or an error occurs.\n","\n"," <font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>Open each notebook and make sure that for every Notebook a default lakehouse has been pinned\n","</font>\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"44e34426-4dec-4f81-b1ae-834ecbfb84eb"},{"cell_type":"markdown","source":["##### Step 1 - Initialise\n","Set the desired number of tables to run this demonstration against. Note if using the default starter pool the maximum is approximately 50 tables, 10 is ideal for demonstration purposes.\n","\n","You may need to re-run this cell below if your session is disconnected as other cells below rely on the numtables variable to be set."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"25274b0b-748c-4afd-998d-4d807201e1c7"},{"cell_type":"code","source":["# Set the number of tables required\n","numtables=5"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":22,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:28:22.2901024Z","session_start_time":null,"execution_start_time":"2024-02-15T18:28:22.7521238Z","execution_finish_time":"2024-02-15T18:28:23.1768545Z","parent_msg_id":"d5b3c3cd-b510-4a7e-9d5d-558c15146d05"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 22, Finished, Available)"},"metadata":{}}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ed6835d3-fc9d-4f72-89bb-92bfb9d66f06"},{"cell_type":"markdown","source":["Run the cell below to set up the tables (will be dropped if exist) and populate sample data in the relative base location. This will take around 5 minutes depending on the number of tables defined."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f521b033-cbd0-4d24-9554-cebdb914f93a"},{"cell_type":"code","source":["\n","# Do not change the relative base - this is where change feed data and checkpoints will be stored\n","relbaselocation = \"Files/AutoMerger/\"\n","\n","from pyspark.sql.functions import col,lit,current_timestamp, input_file_name\n","\n","def create_tables(table_name):\n","  \n","  print(\"Dropping table \"+table_name+\" if exists\")\n","  spark.sql(\"drop table if exists \"+table_name+\";\")\n","  print(\"Creating table \"+table_name + \" from initial load file\")\n","  # Read at least one initial file from the sample. This is to simulate the initial loading process. \n","  # Using the withColumn statement, add the current timestamp and input file name as columns to the table.\n","  df = spark.read.format(\"parquet\").load(relbaselocation+'/basetable/part-00000-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet') \\\n","  .withColumn(\"changeTimestamp\",current_timestamp()).withColumn(\"inputFile\",input_file_name())\n","  df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","print(\"Resetting checkpoint and incremental directories if they exist\")\n","try:\n","  mssparkutils.fs.rm(relbaselocation+\"checkpoints/\",True)\n","except:\n","  None\n","try:\n","  mssparkutils.fs.rm(relbaselocation+\"incrementalfeed/\",True)\n","except:\n","  None\n","\n","\n","# Copy sample data\n","for i in range(0,10):\n","    if not mssparkutils.fs.exists(relbaselocation+\"/basetable/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\"):\n","      print(\"Copying sample data file: \"+ relbaselocation+\"/basetable/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","      mssparkutils.fs.cp(\"https://azuresynapsestorage.blob.core.windows.net/sampledata/WideWorldImportersDW/parquet/incremental/fact_sale_1y_incremental/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\",relbaselocation+\"/basetable/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","\n","# Create tables. Note these will be appear in the tables section with the name \"table\" suffixed with a numerical value \n","for i in range(numtables):\n","  table_name = 'table'+str(i+1)\n","  create_tables(table_name)\n","  \n","  # Copy the first sample of data to the incremental feed folder   \n","  print(\"Copying first incremental file (part-00001) to be processed...\")\n","  mssparkutils.fs.cp(relbaselocation+\"/basetable/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\",relbaselocation+\"/incrementalfeed/\"+table_name+\"/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","\n","print(\"Complete\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"cfa13de6-0250-438e-b391-dee78a9f5d8a","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T17:24:58.0539336Z","session_start_time":"2024-02-15T17:24:58.4643196Z","execution_start_time":"2024-02-15T17:25:06.7528965Z","execution_finish_time":"2024-02-15T17:26:09.2183247Z","parent_msg_id":"342169b6-ee92-4c02-93f9-98bcf3832804"},"text/plain":"StatementMeta(, cfa13de6-0250-438e-b391-dee78a9f5d8a, 3, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Resetting checkpoint and incremental directories if they exist\nCopying sample data file: Files/AutoMerger//basetable/part-00000-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00002-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00003-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00004-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00005-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00006-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00007-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00008-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nCopying sample data file: Files/AutoMerger//basetable/part-00009-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\nDropping table table1 if exists\nCreating table table1 from initial load file\nCopying first incremental file (part-00001) to be processed...\nDropping table table2 if exists\nCreating table table2 from initial load file\nCopying first incremental file (part-00001) to be processed...\nDropping table table3 if exists\nCreating table table3 from initial load file\nCopying first incremental file (part-00001) to be processed...\nDropping table table4 if exists\nCreating table table4 from initial load file\nCopying first incremental file (part-00001) to be processed...\nDropping table table5 if exists\nCreating table table5 from initial load file\nCopying first incremental file (part-00001) to be processed...\nComplete\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"eb96a992-3803-41f1-ab66-0a20e8cbf07a"},{"cell_type":"markdown","source":["##### Step 2 - Run the incremental loading notebook\n","\n","Inspect the incrementalfeed folder (under the base location) for table1 - you should see at least one file, which is a new incremental file (part-00001) waiting for processing. \n","\n","Now navigate to the 02- orchestrator notebook, read the instructions and ensure the first cell has run successfully (if using batch mode) or is running (if using streaming trigger mode) before returning to this notebook.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"59653a01-5305-4d15-9f80-ad426a3d4429"},{"cell_type":"markdown","source":["##### Step 3 - Verify the new incremental file was loaded\n","\n","Run the cell below to verify table1 was affected. Two entries appear in the history, the first was the initial load (write operation) and the second (most recent) is merge operation. You will notice these were all inserted rows, i.e. no updates or deletes occurred."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"afdd657e-fd5f-4ebe-bbb4-28cb63ba76d2"},{"cell_type":"code","source":["#set the table_name variable  \n","table_name = \"table1\"\n","\n","df = spark.sql(\"describe history \"+table_name)\n","display(df.select(\"timestamp\",\"operation\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\",\"operationMetrics.numTargetRowsInserted\",\"operationMetrics.numTargetRowsMatchedUpdated\").orderBy(\"timestamp\",ascending=False))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:04:21.1300829Z","session_start_time":null,"execution_start_time":"2024-02-15T18:04:21.5348151Z","execution_finish_time":"2024-02-15T18:04:23.123693Z","parent_msg_id":"f585a474-56b5-45a0-86aa-9fa2e9ff5d85"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 6, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"8f8d9cb3-4242-4169-8130-e3f477cfa7b2","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 8f8d9cb3-4242-4169-8130-e3f477cfa7b2)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"8d212efa-6684-428e-a062-acbadccac187"},{"cell_type":"markdown","source":["Another way to validate that data was loaded is by the changeTimestamp column..."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"06581d2b-8db6-49da-9a38-ec37f69d3a1b"},{"cell_type":"code","source":["%%sql\n","select changeTimestamp,count(*) from table1 group by changeTimestamp order by 1"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":17,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:18:40.7196942Z","session_start_time":null,"execution_start_time":"2024-02-15T18:18:41.1347382Z","execution_finish_time":"2024-02-15T18:18:43.7145787Z","parent_msg_id":"1e2cb8f5-a505-47cc-b943-d83365a323ce"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 17, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":15,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"changeTimestamp","type":"timestamp","nullable":true,"metadata":{}},{"name":"count(1)","type":"long","nullable":false,"metadata":{"__autoGeneratedAlias":"true"}}]},"data":[["2024-02-15T17:25:25Z","911626"],["2024-02-15T17:54:17Z","1135906"],["2024-02-15T18:14:35Z","500"],["2024-02-15T18:18:21Z","1000"]]},"text/plain":"<Spark SQL result set with 4 rows and 2 fields>"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"e42e8e28-82b2-4aec-becc-491170ebc92e"},{"cell_type":"markdown","source":["##### Step 3a - Configuration additional incremental loads\n","The below cells allow you to simulate incoming incremental files to be merged with the target tables. Each incremental file arrives in an associated sub folder matching the table name. \n","\n","Instructions:\n","1. Once the stream is running or has been run (if using batch mode) in the Orchestrator notebook, set the variables below to simulate the number of inserts and updates. Leave the filepos variable set to 2.\n","2. Run this cell below only once as it sets the filepos variable which determines the starting position for incremental files of which there are 9 in total. \n","3. Then run the cell in Step 3b to add new files to the incremental folder for each table. This increments the filepos variable to fetch the next incremental file.\n","4. Run the orchestrator notebook again (if using batch mode otherwise it should be continuosly running) to load the new incremental data from step 3.\n","5. Monitor the changes to the target table using the table history command in Step 4 to examine the number of rows affected. This should match the number of inserts and updates specified below.\n","6. Repeat steps 3b-5.\n","7. Terminate the running cell in the orchestrator if using streaming mode\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f1efc498-1579-4d1e-8b01-91e452494c5a"},{"cell_type":"code","source":["# Set the number of tables, inserts and updates  \n","numupdates = 500\n","numinserts=500\n","relbaselocation = \"Files/AutoMerger/\"\n","\n","# Do not change this starting file position 2 which is the incremental file starting position. \n","# This value Will be incremented each time the cell below is run to add incremental files\n","filepos = 2\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:12:07.6922712Z","session_start_time":null,"execution_start_time":"2024-02-15T18:12:08.1162407Z","execution_finish_time":"2024-02-15T18:12:08.4278983Z","parent_msg_id":"04b1b795-b5f2-4abc-979e-9a18c82ae0d3"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 10, Finished, Available)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d83040ad-1d16-417b-9b3d-463334a9b800"},{"cell_type":"markdown","source":["##### Step 3b - Load incremental data into the incremental feed per table\n","The below cells generate the specified number of inserts (based on new sample data) and updates (using existing keys from the initial file) and allow you to simulate incoming incremental files to be merged with the target tables. Each incremental file arrives in an associated sub folder matching the table name. \n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"39a69f60-b4e5-46c4-99cc-31b1ca5d3c76"},{"cell_type":"code","source":["from pyspark.sql.functions import col,lit,current_timestamp\n","\n","if filepos<10:\n","  print(\"Adding incremental file #\" + str(filepos) + \" to the incremental feed folder for each associated table.\")\n","  print(\"Please wait until complete...\")\n","  # Obtain a set of already existing records so that these can be updated to form part of the next incremental set of updates\n","  dfupdates = spark.read.format(\"parquet\") \\\n","  .load(relbaselocation+\"/basetable/part-00000-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\") \\\n","  .orderBy('salekey',ascending=True).limit(numupdates)\n","  dfupdates.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/temptable\")\n","  spark.sql(\"update temptable set Description = 'Test update\"+str(filepos)+\"'\")\n","  dfupdates = spark.sql(\"select * from temptable\")\n","\n","  dfinserts = spark.read.format(\"parquet\") \\\n","  .load(relbaselocation+\"/basetable/part-0000\" + str(filepos) + \"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\") \\\n","  .orderBy('salekey',ascending=False).limit(numinserts)\n","  df03 = dfupdates.union(dfinserts)\n","  for p in range(1,numtables+1):\n","    df03.coalesce(1).write.mode(\"append\").format(\"parquet\").save(relbaselocation+\"/incrementalfeed/table\"+str(p))  \n","\n","  filepos = filepos+1\n","  print('Complete.')\n","else:\n","  print(\"No further sample incremental data is available. Please reset the demo by restarting this notebook.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":18,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:19:10.1022768Z","session_start_time":null,"execution_start_time":"2024-02-15T18:19:10.5298615Z","execution_finish_time":"2024-02-15T18:19:27.4188647Z","parent_msg_id":"824a91e3-62bc-4e2d-a009-f469c90ca578"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 18, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Adding incremental file #4 to the incremental feed folder for each associated table.\nPlease wait until complete...\nComplete.\n"]}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6bb93a30-4613-483c-8a3a-4f4bb6ea8e2f"},{"cell_type":"markdown","source":["##### Step 4 - Verify data has been merged into the target table\n","\n","You can verify the number of inserts and updates as well as the latency/timestamp at which these occured."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a1ff425f-538f-455d-b5f5-d5711a0f7c8c"},{"cell_type":"code","source":["#set the table_name variable  \n","table_name = \"table1\"\n","\n","df = spark.sql(\"describe history \"+table_name)\n","display(df.select(\"timestamp\",\"operation\",\"operationMetrics.numTargetRowsInserted\",\"operationMetrics.numTargetRowsMatchedUpdated\").orderBy(\"timestamp\",ascending=False))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":21,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:25:41.3160942Z","session_start_time":null,"execution_start_time":"2024-02-15T18:25:41.9185667Z","execution_finish_time":"2024-02-15T18:25:44.4778212Z","parent_msg_id":"545a8849-9da6-493f-b22b-76fbc0e50831"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 21, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3364ccdb-0c3c-4732-8972-8675a1062de6","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3364ccdb-0c3c-4732-8972-8675a1062de6)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{},"collapsed":false},"id":"723984eb-22a3-46db-a7e7-903924c9e941"},{"cell_type":"markdown","source":["##### Optional: Obtain process wide statistics from all tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0f9b9c9d-1afc-4b5c-bc3a-7fe9fa5f5ee8"},{"cell_type":"code","source":["# Verify the recent inserts and updates for a specific table\n","from delta.tables import *\n","from pyspark.sql.functions import col,lit,current_timestamp\n","\n","spark = SparkSession(sc)\n","table_name = \"table1\"\n","deltaTable = DeltaTable.forPath(spark, \"Tables/\" + table_name)\n","df =  deltaTable.history()\n","dftemp = df.withColumn(\"Table\",lit(table_name))\n","dfhist=dftemp.select(\"Table\",\"timestamp\",\"operationMetrics.numTargetRowsInserted\",\"operationMetrics.numTargetRowsMatchedUpdated\")\n","for p in range(2,numtables+1):\n","    table_name = \"table\"+str(p)\n","    deltaTable = DeltaTable.forPath(spark, \"Tables/\" + table_name)\n","    df =  deltaTable.history()\n","    dftemp = df.withColumn(\"Table\",lit(table_name))\n","    dfhist =dfhist.union(dftemp.select(\"Table\",\"timestamp\",\"operationMetrics.numTargetRowsInserted\",\"operationMetrics.numTargetRowsMatchedUpdated\"))\n","display(dfhist.filter(\"numTargetRowsInserted is not null\").orderBy([\"Table\",\"timestamp\"],ascending=[True,False]))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"33f9b13d-e4c6-4428-bb99-4af607609ee4","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-15T18:17:06.5565058Z","session_start_time":null,"execution_start_time":"2024-02-15T18:17:06.9894011Z","execution_finish_time":"2024-02-15T18:17:09.5179978Z","parent_msg_id":"e9c6626a-99e4-4000-8c44-4016e1edc0d5"},"text/plain":"StatementMeta(, 33f9b13d-e4c6-4428-bb99-4af607609ee4, 15, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"e11f4838-1ac3-471d-bce7-fd82b67b81ae","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, e11f4838-1ac3-471d-bce7-fd82b67b81ae)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{},"collapsed":false},"id":"36715473-556a-430f-bb22-6a8802a712ce"},{"cell_type":"markdown","source":["##### House-keeping\n","\n","1. Terminate the running cell in the orchestrator notebook if using streaming mode.\n","2. If you ran the orchestrator notebook in streaming mode then ensure no active queries exist\n","3. Run the cells below to remove all demo files and tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"61f6ba15-dbd5-4f85-ab64-fb1b19e41d78"},{"cell_type":"code","source":["def drop_tables(tablenum):\n","  table_name = 'table'+str(tablenum+1)\n","  print(\"Dropping table \"+table_name+\" if exists\")\n","  spark.sql(\"drop table if exists \"+table_name+\";\")\n","  # Copy the first sample data to the incremental feed folder   \n","  mssparkutils.fs.cp(relbaselocation+\"/basetable/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\",relbaselocation+\"/incrementalfeed/\"+table_name+\"/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","\n","print(\"Cleaning up...\")\n","for i in range(numtables):\n","  drop_tables(i)\n","\n","print(\"Removing base location files...\")\n","try:\n","    mssparkutils.fs.rm(relbaselocation,True)\n","except:\n","    None\n","print(\"Done\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"89f6fa19-3cec-4847-8814-cd0c49e74e3a"},{"cell_type":"markdown","source":["##### Troubleshooting\n","\n","If you encounter an error which suggests that the stream for a particular table is still active, you should copy the cell below into the orchestrator notebook and run this to terminate any active queries"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"adf4a3b1-5f9b-4adc-905d-67f2fb273e91"},{"cell_type":"code","source":["import time\n","# Helper method to stop a streaming query\n","def stop_stream_query(query, wait_time):\n","    \"\"\"Stop a running streaming query\"\"\"\n","    while query.isActive:\n","        msg = query.status['message']\n","        data_avail = query.status['isDataAvailable']\n","        trigger_active = query.status['isTriggerActive']\n","        if not data_avail and not trigger_active and msg != \"Initializing sources\":\n","            print('Stopping query...')\n","            query.stop()\n","        time.sleep(0.5)\n","\n","    # Okay wait for the stop to happen\n","    print('Awaiting termination...')\n","    query.awaitTermination(wait_time)\n","\n","sqm = spark.streams\n","for q in sqm.active:\n","  print(q.name + \"query is still active, terminating...\")\n","  stop_stream_query(q,2000)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"995748d2-92bc-40c2-b016-f12681b25d1f"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"8f8d9cb3-4242-4169-8130-e3f477cfa7b2":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2024-02-15 17:55:11.299","1":"MERGE","2":"NULL","3":"1135906","4":"1135906","5":"0","index":"1"},{"0":"2024-02-15 17:25:32.314","1":"WRITE","2":"1","3":"912126","4":"NULL","5":"NULL","index":"2"}],"schema":[{"key":"0","name":"timestamp","type":"timestamp"},{"key":"1","name":"operation","type":"string"},{"key":"2","name":"numFiles","type":"string"},{"key":"3","name":"numOutputRows","type":"string"},{"key":"4","name":"numTargetRowsInserted","type":"string"},{"key":"5","name":"numTargetRowsMatchedUpdated","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"e11f4838-1ac3-471d-bce7-fd82b67b81ae":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"table1","1":"2024-02-15 18:14:53.621","2":"500","3":"500","index":"1"},{"0":"table1","1":"2024-02-15 17:55:11.299","2":"1135906","3":"0","index":"2"},{"0":"table2","1":"2024-02-15 18:14:50.34","2":"500","3":"500","index":"3"},{"0":"table2","1":"2024-02-15 17:55:14.416","2":"1135906","3":"0","index":"4"},{"0":"table3","1":"2024-02-15 18:14:53.229","2":"500","3":"500","index":"5"},{"0":"table3","1":"2024-02-15 17:55:11.656","2":"1135906","3":"0","index":"6"},{"0":"table4","1":"2024-02-15 18:14:48.724","2":"500","3":"500","index":"7"},{"0":"table4","1":"2024-02-15 17:55:11.453","2":"1135906","3":"0","index":"8"},{"0":"table5","1":"2024-02-15 18:14:51.353","2":"500","3":"500","index":"9"},{"0":"table5","1":"2024-02-15 17:55:15.531","2":"1135906","3":"0","index":"10"}],"schema":[{"key":"0","name":"Table","type":"string"},{"key":"1","name":"timestamp","type":"timestamp"},{"key":"2","name":"numTargetRowsInserted","type":"string"},{"key":"3","name":"numTargetRowsMatchedUpdated","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"3364ccdb-0c3c-4732-8972-8675a1062de6":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2024-02-15 18:19:35.274","1":"MERGE","2":"500","3":"500","index":"1"},{"0":"2024-02-15 18:18:31.265","1":"MERGE","2":"500","3":"500","index":"2"},{"0":"2024-02-15 18:14:51.353","1":"MERGE","2":"500","3":"500","index":"3"},{"0":"2024-02-15 17:55:15.531","1":"MERGE","2":"1135906","3":"0","index":"4"},{"0":"2024-02-15 17:26:06.582","1":"WRITE","2":"NULL","3":"NULL","index":"5"}],"schema":[{"key":"0","name":"timestamp","type":"timestamp"},{"key":"1","name":"operation","type":"string"},{"key":"2","name":"numTargetRowsInserted","type":"string"},{"key":"3","name":"numTargetRowsMatchedUpdated","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["2"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"79e37988-3371-4daa-8759-94e349be4cf6","default_lakehouse_name":"Raw","default_lakehouse_workspace_id":"7be392e2-d65b-45f0-83f4-7d82ffb36cc0"}}},"nbformat":4,"nbformat_minor":5}