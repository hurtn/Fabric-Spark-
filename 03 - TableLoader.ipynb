{"cells":[{"cell_type":"code","source":["pTableName = 'table1'\n","pJoinKey= 'salekey'\n","pOrderKey=''\n","pTriggerType = \"batch\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ea568abd-cea4-4d6a-b05b-d4a5abc7842b","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-16T18:19:52.2526696Z","session_start_time":null,"execution_start_time":"2024-01-16T18:19:52.646273Z","execution_finish_time":"2024-01-16T18:19:52.9489657Z","parent_msg_id":"40ab8a58-58bf-4877-a4df-8a50f89e45af"},"text/plain":"StatementMeta(, ea568abd-cea4-4d6a-b05b-d4a5abc7842b, 11, Finished, Available)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"0d2ac7ad-f1d4-4dd5-8610-e6bd2f3fa9b3"},{"cell_type":"markdown","source":["###### Explaination \n","\n","Thanks to Spark Structured Streaming checkpointing, the cell below detects any files for processing since the last run. It uses an overloaded writestream method known as a foreachbatch statement to perform a SQL merge statement against the target table. For more information on foreachbatch and streaming queries please see [this section](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch) of the Apache Spark documentation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"956be0c6-73bc-4d31-b447-8dddd890c32a"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import col,lit,current_timestamp, input_file_name\n","import sys\n","\n","spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n","spark.sql(\"set spark.streaming.stopGracefullyOnShutdown=true\")\n","\n","relbaselocation = \"Files/AutoMerger\"\n","\n","def loadIncrementals(table_name, join_key, order_key):\n","    printlog = \"\"\n","    def upsertToDeltaCaptureCDC(microBatchOutputDF, batchId):\n","        updatestmt = ''\n","        insertsrcstmt = ''\n","        inserttgtstmt = ''\n","        targettable = microBatchOutputDF.select(\"fep_table_name\").limit(1).collect()[0][0]\n","        joinkey = microBatchOutputDF.select(\"fep_join_key\").limit(1).collect()[0][0]\n","        orderkey = microBatchOutputDF.select(\"fep_order_key\").limit(1).collect()[0][0]\n","        #add this to the end of the next line if you require a timestmape field .withColumn(\"changeTimestamp\",current_timestamp())\n","        microBatchOutputDF = microBatchOutputDF.drop(\"fep_join_key\").drop('fep_table_name').withColumn(\"changeTimestamp\",current_timestamp()).withColumn(\"inputFile\",input_file_name())\n","        microBatchOutputDF.createOrReplaceTempView(\"updates\")\n","        tablecols = spark.sql('show columns in '+targettable).collect()\n","\n","        for column in tablecols:\n","          updatestmt = updatestmt+column.col_name +'= source.'+column.col_name + ','\n","          insertsrcstmt = insertsrcstmt + column.col_name+ ','\n","          inserttgtstmt = inserttgtstmt + \"source.\"+column.col_name+ ','\n","\n","        # These columns would only be necessary if we didn't hard code the last column such as timestamp above\n","        updatestmt = updatestmt.rstrip(\",\")\n","        insertsrcstmt = insertsrcstmt.rstrip(\",\")\n","        inserttgtstmt = inserttgtstmt.rstrip(\",\")\n","\n","        if orderkey != '':\n","          mergestmt = '''MERGE INTO '''+targettable+''' target\n","          USING (SELECT X.* FROM\n","                (\n","                SELECT\n","                    DENSE_RANK() OVER (PARTITION BY CT.'''+joinkey+''' ORDER BY CT.'''+orderkey+''' desc) AS DR\n","                ,   CT.*\n","                FROM\n","                    updates AS CT\n","                ) X\n","                where DR=1\n","                )  source \n","          ON source.'''+ joinkey + '''=  target.'''+joinkey+'''\n","          WHEN MATCHED THEN UPDATE SET ''' + updatestmt + '''\n","          WHEN NOT MATCHED THEN INSERT (''' + insertsrcstmt + ''') VALUES ('''+inserttgtstmt + ''')'''\n","        else:\n","          mergestmt = '''MERGE INTO '''+targettable+''' target\n","          USING updates AS source \n","          ON source.'''+ joinkey + '''=  target.'''+joinkey+'''\n","          WHEN MATCHED THEN UPDATE SET ''' + updatestmt + '''\n","          WHEN NOT MATCHED THEN INSERT (''' + insertsrcstmt + ''') VALUES ('''+inserttgtstmt + ''')'''\n","        #printlog = printlog + \"| \" + mergestmt\n","        microBatchOutputDF.sparkSession.sql(mergestmt)\n","  \n","    printlog = printlog + \"| Reading new data from location: \"+ relbaselocation+\"/incrementalfeed/\"+table_name+\"/\"\n","    streamingUpserts = spark.readStream.format(\"parquet\").load(relbaselocation+\"/incrementalfeed/\"+table_name+\"/\").withColumn(\"fep_table_name\", lit(table_name)) \\\n","      .withColumn(\"fep_join_key\", lit(join_key)).withColumn(\"fep_order_key\", lit(order_key)) \n","\n","    # Use  .trigger(availableNow=True) for running in scheduled mode and remove awaitTermination statement if you want to run this as a streaming batch.\n","    if pTriggerType == 'batch':\n","        printlog = printlog + \"| Running in batch mode\"\n","        query = streamingUpserts.writeStream.queryName(table_name).format(\"delta\").foreachBatch(upsertToDeltaCaptureCDC).trigger(availableNow=True).option(\"checkpointLocation\",relbaselocation+\"/checkpoints/\"+table_name+\"/\").start()\n","\n","    else:\n","        printlog = printlog + \"| Running in streaming mode\"\n","        query = streamingUpserts.writeStream.queryName(table_name).format(\"delta\").foreachBatch(upsertToDeltaCaptureCDC).option(\"checkpointLocation\",relbaselocation+\"/checkpoints/\"+table_name+\"/\").start()\n","\n","    try:\n","      query.awaitTermination()\n","    except Exception as error:\n","      #mssparkutils.notebook.exit(\"Error loading \"+ pTableName + \". Print log:\"+ printlog + \". Error log: \" +  str(error))\n","      raise\n","\n","    if pTriggerType == 'batch':\n","      mssparkutils.notebook.exit(\"Successfully loaded \"+ pTableName + \". Print log:\"+ printlog)\n","    \n","loadIncrementals(pTableName,pJoinKey,pOrderKey)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ea568abd-cea4-4d6a-b05b-d4a5abc7842b","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-16T18:19:52.3759351Z","session_start_time":null,"execution_start_time":"2024-01-16T18:19:53.4247434Z","execution_finish_time":"2024-01-16T18:20:12.2197309Z","parent_msg_id":"9484ce7d-fe96-444b-be18-17b8179c82a9"},"text/plain":"StatementMeta(, ea568abd-cea4-4d6a-b05b-d4a5abc7842b, 12, Finished, Available)"},"metadata":{}}],"execution_count":10,"metadata":{},"id":"b836cc38-4ad1-45c2-b5c8-b2493cb8d3c9"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"trident":{"lakehouse":{"default_lakehouse":"79e37988-3371-4daa-8759-94e349be4cf6","default_lakehouse_name":"Raw","default_lakehouse_workspace_id":"7be392e2-d65b-45f0-83f4-7d82ffb36cc0"},"environment":{}}},"nbformat":4,"nbformat_minor":5}