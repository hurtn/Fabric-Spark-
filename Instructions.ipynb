{"cells":[{"cell_type":"markdown","source":["#### Setup Instructions\n","##### Step 1 - Initialise\n","Set the number of tables required and the relative base location where the sample data and checkpoints are stored\n","\n","Run the cell below to set up the tables and sample relative base location <p>\n","\n","Remember to add the target lakehouse and set as default\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"25274b0b-748c-4afd-998d-4d807201e1c7"},{"cell_type":"code","source":["# Set the number of tables required\n","numtables=5\n","# Set the relative base path for sample data\n","relbaselocation = \"Files/AutoMerger/\"\n","\n","def create_tables(tablenum):\n","  table_name = 'table'+str(tablenum+1)\n","  print(\"Dropping table \"+table_name+\" if exists\")\n","  spark.sql(\"drop table if exists table\"+table_name+\";\")\n","  print(\"Loading and creating table \"+table_name)\n","  df = spark.read.format(\"parquet\").load(relbaselocation+'/basetable/part-00000-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet')\n","  df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","  print(\"Resetting checkpoint and incremental directories\")\n","  try:\n","    mssparkutils.fs.rm(\"Files/checkpoints/\"+table_name+\"\",True)\n","  except:\n","    None\n","  try:\n","    mssparkutils.fs.rm(\"Files/incremental/\"+table_name+\"\",True)\n","  except:\n","    None\n","  # Copy the first sample data to the incremental feed folder   \n","  mssparkutils.fs.cp(relbaselocation+\"/basetable/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\",relbaselocation+\"/incrementalfeed/\"+table_name+\"/part-00001-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","\n","\n","# Copy sample data\n","for i in range(0,10):\n","    if not mssparkutils.fs.exists(relbaselocation+\"/basetable/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\"):\n","      print(\"Copying sample data file: \"+ relbaselocation+\"/basetable/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","      mssparkutils.fs.cp(\"https://azuresynapsestorage.blob.core.windows.net/sampledata/WideWorldImportersDW/parquet/incremental/fact_sale_1y_incremental/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\",relbaselocation+\"/basetable/part-0000\"+str(i)+\"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\")\n","# Create tables. Note these will be appear in the tables section with the name \"table\" suffixed with a numerical value \n","for i in range(numtables):\n","  create_tables(i)\n","print(\"Complete\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"eb96a992-3803-41f1-ab66-0a20e8cbf07a"},{"cell_type":"markdown","source":["#### Step 2\n","\n","Now navigate to the Orchestrator notebook, read the instructions and run.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"59653a01-5305-4d15-9f80-ad426a3d4429"},{"cell_type":"markdown","source":["##### Step 3 - Prepare and load incremental data into the incremental feed per table\n","Set the variables below"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f1efc498-1579-4d1e-8b01-91e452494c5a"},{"cell_type":"code","source":["# Set the number of tables, inserts and updates  \n","numupdates = 500\n","numinserts=500\n","numtables=5\n","\n","# Starting incremental feed at file position 2. Will be incremented each time the cell below is run\n","filepos = 2\n","\n","relbaselocation = \"Files/AutoMerger/\"\n","\n","print(\"Loading incremental file position \" + str(filepos))\n","\n","# Obtain a set of already existing records so that these can be updated to form part of the next incremental batch \n","dfupdates = spark.read.format(\"parquet\") \\\n",".load(relbaselocation+\"/basetable/part-00000-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\") \\\n",".orderBy('salekey',ascending=True).limit(numupdates)\n","dfupdates.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/temptable\")\n","spark.sql(\"update temptable set Description = 'Test update\"+str(filepos)+\"'\")\n","dfupdates = spark.sql(\"select * from temptable\")\n","\n","dfinserts = spark.read.format(\"parquet\") \\\n",".load(relbaselocation+\"/basetable/part-0000\" + str(filepos) + \"-5eaa21a0-54da-459d-b098-307a78d5d41e-c000.snappy.parquet\") \\\n",".orderBy('salekey',ascending=False).limit(numinserts)\n","df03 = dfupdates.union(dfinserts)\n","for p in range(1,numtables):\n","  df03.coalesce(1).write.mode(\"append\").format(\"parquet\").save(relbaselocation+\"/incrementalfeed/table\"+str(p))  \n","\n","filepos = filepos+1"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6bb93a30-4613-483c-8a3a-4f4bb6ea8e2f"},{"cell_type":"markdown","source":["###### Step 4 - Verify data has been merged into the target tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a1ff425f-538f-455d-b5f5-d5711a0f7c8c"},{"cell_type":"code","source":["# Verify the recent inserts and updates for a specific table\n","df = spark.sql(\"describe history table1\")\n","display(df.select(\"timestamp\",\"operationMetrics.numTargetRowsInserted\",\"operationMetrics.numTargetRowsMatchedUpdated\").orderBy(\"timestamp\",ascending=False))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{},"collapsed":false},"id":"36715473-556a-430f-bb22-6a8802a712ce"},{"cell_type":"code","source":["'''\n","Issue to resolve\n","\n","Cannot perform Merge as multiple source rows matched and attempted to modify the same\n","target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n","when multiple source rows match on the same target row, the result may be ambiguous\n","as it is unclear which source row should be used to update or delete the matching\n","target row. You can preprocess the source table to eliminate the possibility of\n","multiple matches. Please refer to\n","https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge'''"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e8c996d4-c588-4bba-97af-a95a126e088b"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"known_lakehouses":[{"id":"518c0d45-cc85-4f38-a9f7-09909df6759e"}],"default_lakehouse":"518c0d45-cc85-4f38-a9f7-09909df6759e","default_lakehouse_name":"Aqua","default_lakehouse_workspace_id":"1a2e0f08-2ece-4d91-8bf7-7104887e3bbc"}}},"nbformat":4,"nbformat_minor":5}